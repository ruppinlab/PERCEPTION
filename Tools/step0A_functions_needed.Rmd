---
title: "R Notebook"
output: html_notebook
---
<!-- These are functions are custom functions that are needed to perform  -->
<!-- simple tasks eg. rank nomalization of a mat, error handling etc. -->
<!-- The is to avoid repetitively writing same scripts are used many times -->
```{r}
# rank normalize a matrix
rank_normalization_mat <- function(mat){
  apply(mat, 2, function(x) rank(x, ties.method = "average")/length(x))
}

# Count the number of NAs in each row of a matrix
count_row_NAs<-function(df){
  apply(df, 1, function(x) sum(is.na(x)))
}

# Handing a error when a function is run and returning NA in case of error
# instead of stopping the task.
err_handle<-function(x){ tryCatch(x, error=function(e){NA}) }

# Custom Head which only first returns 5 rows and columns
myhead<-function(mat){
  mat[1:min(5, nrow(mat)), 1:min(5, ncol(mat))]
}

hypergeometric_test_for_twolists<-function(test_list, base_list, global, infunc_lowertail=FALSE) {
    # If you sample X balls from a bag of white and black balls mixture
  # the function provides the probability of observing Y or greater white balls.
  # Above test_list refers to white balls
  #If lowertail=TRUE - we calculate the probability for "Y or less white balls"
  base_list_within_global=global[na.omit(match(base_list, global))]
  intersect_of_two_list= test_list[!is.na(match(test_list, base_list_within_global))]
  phyper(length(intersect_of_two_list)-1, #white balls in the samples
         length(base_list_within_global), #total white balls in the box
         length(global)- length(base_list_within_global), #total black balls in the box
         length(test_list), #total balls sampled
         lower.tail=infunc_lowertail) #whether you wish to calculate enrichment or depletion
}

# FDR correction
fdrcorr<-function(test_list){p.adjust(test_list, method = 'fdr')}

#Subsetting a set of columns
colSubset<-function(mat, column_Names){
  mat[,na.omit(match(column_Names, colnames(mat)))]
}

#Subsetting a set of rows
rowSubset<-function(mat, row_Names){
  mat[na.omit(match(row_Names, rownames(mat))),]
}

# Given a package, this functions either loads it, and in case it is not installed,
# it installs and then load it
installORload<-function(packages){
  package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  })
}

# Change range to 0-1
range01 <- function(x){
  # Chossing 95% and 5% percentile as thresholds for outliers
  substitute_of_Min=topXPercentValue(vec=x, 
                                     X_percentile=5)
  substitute_of_Max=topXPercentValue(vec=x, 
                                     X_percentile=95)
  x_scaled=(x-substitute_of_Min)/(substitute_of_Max-substitute_of_Min)
  x_scaled[x_scaled<0]=0
  x_scaled[x_scaled>1]=1
  x_scaled
}

# topXPercentValue of a vector
topXPercentValue<-function(vec, X_percentile=95){
  vec=na.omit(vec)
  len=length(vec)
  vec=sort(vec)
  vec[ceiling(len*(X_percentile/100))]
}

# Taking vector as an input, it converts the factor vector into numeric
factor2numeric<-function(x){
  as.numeric(as.character(x))
}

# Strip all non-char and non-numeric and make lower case
# this is primarily to facilitate inconsistent naming comparison (eg. drugNames)
stripall2match<-function(x){
  tolower(gsub('[^A-z0-9]','',x) )
}

# Opposite of standard %in% function
'%!in%' <- function(x,y)!('%in%'(x,y))

# Split a string and return the item of interest (retreving_onject_id)
strsplit_customv0 <- function(infunc_list=pred_viab$cellLines_mapping$cellLine_ID,
                              infunc_split_by='_',
                              retreving_onject_id=1){
  sapply(strsplit(infunc_list, split = infunc_split_by), function(x) x[retreving_onject_id])
}

# Compute a colMax
colMax <- function (colData) {
  apply(colData, MARGIN=c(2), max)
}
# Compute a colMedian
colMedian <- function (colData) {
  apply(colData, MARGIN=c(2), median)
}
# Compute a colMin
colMin <- function (colData) {
  apply(colData, MARGIN=c(2), min)
}

# Compute a RowMax
rowMax <- function (colData) {
  apply(colData, MARGIN=c(1), max)
}
# Compute a RowMin
rowMin <- function (colData) {
  apply(colData, MARGIN=c(1), min)
}

# Load a .RMD file in another file
ksource <- function(x, ...) {
  library(knitr)
  source(purl(x, output = tempfile()), ...)
}

# <!-- Capping a matrix to value 1 -->
capping_at1 <- function(x){
  x[x>1]=1
  x
}
capping_at0 <- function(x){
  x[x<0]=0
  x
}

# RowMeans functions considering the boundary case where mat only has one row
rowMeans_if_one_row <- function(mat){
  if(ncol(mat)>1){
    return(rowMeans(mat))
  } else {
    return(mat)
  }
}
```
<!-- # This is a fast version of correaltion test where only the estiamte and p-value are computed -->
<!-- # Version 0 of a fast R cor.test function  -->
```{r}
cor.test_trimmed_v0 <- function(x, ...) UseMethod("cor.test_trimmed_v0")

cor.test_trimmed_v0.default <-
  function(x, y, alternative = c("two.sided", "less", "greater"),
           method = c("pearson"), exact = NULL,
           # conf.level = 0.95,
           continuity = FALSE, ...)
  {
    alternative <- match.arg(alternative)
    method <- match.arg(method)
    DNAME <- paste(deparse(substitute(x)), "and", deparse(substitute(y)))
    
    if(length(x) != length(y))
      stop("'x' and 'y' must have the same length")
    if(!is.numeric(x)) stop("'x' must be a numeric vector")
    if(!is.numeric(y)) stop("'y' must be a numeric vector")
    OK <- complete.cases(x, y)
    x <- x[OK]
    y <- y[OK]
    n <- length(x)
    
    NVAL <- 0
    # conf.int <- FALSE
    
    if(method == "pearson") {
      if(n < 3L)
        stop("not enough finite observations")
      method <- "Pearson's product-moment correlation"
      names(NVAL) <- "correlation"
      r <- cor(x, y)
      df <- n - 2L
      ESTIMATE <- c(cor = r)
      PARAMETER <- c(df = df)
      STATISTIC <- c(t = sqrt(df) * r / sqrt(1 - r^2))
      # Do not compute confidence Int
      # if(n > 3) { ## confidence int.
      #   if(!missing(conf.level) &&
      #      (length(conf.level) != 1 || !is.finite(conf.level) ||
      #       conf.level < 0 || conf.level > 1))
      #     stop("'conf.level' must be a single number between 0 and 1")
      #   conf.int <- TRUE
      #   z <- atanh(r)
      #   sigma <- 1 / sqrt(n - 3)
      #   cint <-
      #     switch(alternative,
      #            less = c(-Inf, z + sigma * qnorm(conf.level)),
      #            greater = c(z - sigma * qnorm(conf.level), Inf),
      #            two.sided = z +
      #              c(-1, 1) * sigma * qnorm((1 + conf.level) / 2))
      #   cint <- tanh(cint)
      #   attr(cint, "conf.level") <- conf.level
      # }
      PVAL <- switch(alternative,
                     "less" = pt(STATISTIC, df),
                     "greater" = pt(STATISTIC, df, lower.tail=FALSE),
                     "two.sided" = 2 * min(pt(STATISTIC, df),
                                           pt(STATISTIC, df, lower.tail=FALSE)))
    }
    else {
      if(n < 2)
        stop("not enough finite observations")
      PARAMETER <- NULL
      TIES <- (min(length(unique(x)), length(unique(y))) < n)
      if(method == "kendall") {
        method <- "Kendall's rank correlation tau"
        names(NVAL) <- "tau"
        r <- cor(x,y, method = "kendall")
        ESTIMATE <- c(tau = r)
        
        if(!is.finite(ESTIMATE)) {  # all x or all y the same
          ESTIMATE[] <- NA
          STATISTIC <- c(T = NA)
          PVAL <- NA
        }
        else {
          if(is.null(exact))
            exact <- (n < 50)
          if(exact && !TIES) {
            q <- round((r + 1) * n * (n - 1) / 4)
            STATISTIC <- c(T = q)
            pkendall <- function(q, n) .Call(C_pKendall, q, n)
            PVAL <-
              switch(alternative,
                     "two.sided" = {
                       if(q > n * (n - 1) / 4)
                         p <- 1 - pkendall(q - 1, n)
                       else
                         p <- pkendall(q, n)
                       min(2 * p, 1)
                     },
                     "greater" = 1 - pkendall(q - 1, n),
                     "less" = pkendall(q, n))
          } else {
            xties <- table(x[duplicated(x)]) + 1
            yties <- table(y[duplicated(y)]) + 1
            T0 <- n * (n - 1)/2
            T1 <- sum(xties * (xties - 1))/2
            T2 <- sum(yties * (yties - 1))/2
            S <- r * sqrt((T0 - T1) * (T0 - T2))
            v0 <- n * (n - 1) * (2 * n + 5)
            vt <- sum(xties * (xties - 1) * (2 * xties + 5))
            vu <- sum(yties * (yties - 1) * (2 * yties + 5))
            v1 <- sum(xties * (xties - 1)) * sum(yties * (yties - 1))
            v2 <- sum(xties * (xties - 1) * (xties - 2)) *
              sum(yties * (yties - 1) * (yties - 2))
            
            var_S <- (v0 - vt - vu) / 18 +
              v1 / (2 * n * (n - 1)) +
              v2 / (9 * n * (n - 1) * (n - 2))
            
            if(exact && TIES)
              warning("Cannot compute exact p-value with ties")
            if (continuity) S <- sign(S) * (abs(S) - 1)
            STATISTIC <- c(z = S / sqrt(var_S))
            PVAL <- switch(alternative,
                           "less" = pnorm(STATISTIC),
                           "greater" = pnorm(STATISTIC, lower.tail=FALSE),
                           "two.sided" = 2 * min(pnorm(STATISTIC),
                                                 pnorm(STATISTIC, lower.tail=FALSE)))
          }
        }
      } else {
        method <- "Spearman's rank correlation rho"
        if (is.null(exact))
          exact <- TRUE
        names(NVAL) <- "rho"
        r <- cor(rank(x), rank(y))
        ESTIMATE <- c(rho = r)
        if(!is.finite(ESTIMATE)) {  # all x or all y the same
          ESTIMATE[] <- NA
          STATISTIC <- c(S = NA)
          PVAL <- NA
        }
        else {
          ## Use the test statistic S = sum(rank(x) - rank(y))^2
          ## and AS 89 for obtaining better p-values than via the
          ## simple normal approximation.
          ## In the case of no ties, S = (1-rho) * (n^3-n)/6.
          pspearman <- function(q, n, lower.tail = TRUE) {
            if(n <= 1290 && exact) # n*(n^2 - 1) does not overflow
              .Call(C_pRho, round(q) + 2*lower.tail, n, lower.tail)
            else { # for large n: asymptotic t_{n-2}
              den <- (n*(n^2-1))/6 # careful for overflow
              ## Kendall et all (1939) p. 260
              if (continuity) den <- den + 1
              r <- 1 - q/den
              pt(r / sqrt((1 - r^2)/(n-2)), df = n-2,
                 lower.tail = !lower.tail)
            }
          }
          q <- (n^3 - n) * (1 - r) / 6
          STATISTIC <- c(S = q)
          if(TIES && exact){
            exact <- FALSE
            warning("Cannot compute exact p-value with ties")
          }
          PVAL <-
            switch(alternative,
                   "two.sided" = {
                     p <- if(q > (n^3 - n) / 6)
                       pspearman(q, n, lower.tail = FALSE)
                     else
                       pspearman(q, n, lower.tail = TRUE)
                     min(2 * p, 1)
                   },
                   "greater" = pspearman(q, n, lower.tail = TRUE),
                   "less" = pspearman(q, n, lower.tail = FALSE))
        }
      }
    }
    
    RVAL <- list(
      # statistic = STATISTIC,
      # parameter = PARAMETER,
      p.value = as.numeric(PVAL),
      estimate = ESTIMATE
      # ,
      # null.value = NVAL,
      # alternative = alternative,
      # method = method,
      # data.name = DNAME
    )
    # if(conf.int)
    #   RVAL <- c(RVAL, list(conf.int = cint))
    class(RVAL) <- "htest"
    RVAL
  }

cor.test.formula <-
  function(formula, data, subset, na.action, ...)
  {
    if(missing(formula)
       || !inherits(formula, "formula")
       || length(formula) != 2L)
      stop("'formula' missing or invalid")
    m <- match.call(expand.dots = FALSE)
    if(is.matrix(eval(m$data, parent.frame())))
      m$data <- as.data.frame(data)
    ## need stats:: for non-standard evaluation
    m[[1L]] <- quote(stats::model.frame)
    m$... <- NULL
    mf <- eval(m, environment(formula))
    if(length(mf) != 2L)
      stop("invalid formula")
    DNAME <- paste(names(mf), collapse = " and ")
    names(mf) <- c("x", "y")
    y <- do.call("cor.test_trimmed_v0", c(mf, list(...)))
    y$data.name <- DNAME
    y
  }

```
<!-- Libraries needed -->
```{r}
library(RColorBrewer)
library(ggplot2)
library(ggpubr)
library(gridExtra)
library(grid)
library(lattice)

require(data.table,quietly = T)
require('tictoc',quietly = T)
require(ggplot2,quietly = T)
require(statar,quietly = T)
require('parallel',quietly = T)
require('glmnet',quietly = T)
require('caret',quietly = T)
require('randomForest',quietly = T)
require(tidyr,quietly = T)
require(ggpubr)
require(gridExtra)
require(ggrepel)
require(pROC)
library(scales)
library(dplyr)
library(viridis)
library(reshape2)
require(Seurat)
library(corrplot)
require(DEGreport)
require(devtools)
require(ggpmisc)
require(modes)
require(readxl)
```


Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

